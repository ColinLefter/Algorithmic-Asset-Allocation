{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Colin Lefter"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Research question/interests\n",
    "\n",
    "**What equity data is the most deterministic of the price of an equity, and of this data, which is the most relevant for a growth portfolio investment strategy such that we can compute an optimized portfolio of equities while using user input to drive our optimization algorithm?**\n",
    "\n",
    "My research objective is to develop a scalable asset allocation and construction algorithm that implements an objected-oriented design approach. This objective is an outcome of determining what equity data is the most deterministic of the price of an equity, which will be the focus for the majority of the project/\n",
    "\n",
    "I intend to develop algorithms for constructing multiple linear regressions and Fourier Franforms, among others, that I will then use to construct interactive and statistical models with Plotly and Seaborn. As such, I have a strong interest in the system design of our software and in developing helper functions that can assist all of us with processing data more efficiently. I am also looking forward to using Facebook Prophet[^1] to construct a time series forecast of a sample portfolio recommendation from our software, which can be included in our Tableau Dashboard.\n",
    "\n",
    "### Analysis Plan\n",
    "Our objective function is one that takes in a selection of columns from our data sets to then search for the top n companies that satisfy a criteria for having the highest probability of producing an optimal return on investment. These inputs themselves refer to sub-objective functions that take as input user-defined parameters and thresholds that set the criteria for favourable performance attributes. To rank the companies from our data set, and ultimately determine what portion of capital to assign to each equity, I propose a data normalization algorithm that normalizes the data that comprises the favourable subset from each column of our data set. We interpret these normalized values as probabilities of equity selection and ultimately average the score of each company across all columns to then multiply the final score percentage of each company with the total capital specified by the user. In a broad sense, our software is composed of four general classes that include \"Data\", \"Quantitative Analysis\", \"Data Visualization\" and \"Portfolio Construction\". We inherit the properties from each of these classes to build a functional data analysis chain.\n",
    "\n",
    "Our data visualization will be concerned with analyzing the influence of certain financial variables, such as Price-to-Earnings, on the price of each equity from a sample of 500 equities (from the S&P 500 index). Such analysis would begin with a statistical summary that will constitute exploratory data analysis, followed by our application of analysis algorithms that we design. The construction of a portfolio is a bonus of our project and will be made possible by the analysis algorithms we have constructed.\n",
    "\n",
    "**Important Note**\n",
    "A component of the analysis will involve the comparison of different values of financial variables with the corresponding price of each equity. This constitutes inferential analysis as we are attempting to identify a correlation on the basis of picking stocks based on expected performance. Therefore, this will require us to use past financial data and compare this data with the current price of each equity. As a result, we can only use the 3-month performance data (i.e. 3-month change in share price data) for this comparison as otherwise we would be using future data to predict past performance, which would be invalid.\n",
    "\n",
    "#### User-defined parameters\n",
    "Some initial ideas for these parameters include:\n",
    "- (float) Initial capital\n",
    "- (float) Additional capital per day, week or month\n",
    "- (int) Intended holding period (in days)\n",
    "- (boolean) Importance of dividends (validated based on capital invested)\n",
    "- (String) Preferred industries (choose from a list, or select all)\n",
    "- (int) Volatility tolerance (from 0 to 1, 1 indicating that volatility is not important)\n",
    "- (String) Preferred companies (as a list)[^2]\n",
    "- (int) Preferred degree of portfolio diversification (from 0 to 1, 1 indicating complete diversification)\n",
    "- (String) Preferred investment strategy (choose from \"Growth\", \"Value\", \"GARP\")\n",
    "\n",
    "### Algorithm Plan\n",
    "\n",
    "####  Tier 1: Threshold-based screening algorithms\n",
    "- The current plan is to use these algorithms to screen the financial documents from each company by setting a minimum threshold for each financial ratio. This class of algorithms will need to conduct such screening per industry as industry financial ratios are dinstinct from one another.\n",
    "- A global screening algorithm that selects companies which show favourable performance across all ratios can also be used after each ratio has been individually tested.\n",
    "\n",
    "#### Tier 2: Regression models\n",
    "- As of now, the intent is to develop a multiple linear regression model that will attempt to determine a relationship between the yearly and quarterly performance of each company in relation to several columns of data that act as predictors. This can essentially implement the results from the threshold-based screening algorithms to only conduct this analysis on the pre-screened companies.\n",
    "\n",
    "#### Tier 3: Statistical modelling algorithms\n",
    "- Tier 3 denotes a class of broadly experimental statistical modelling algorithms that are applied on a pre-final portfolio to add additional points to companies that perform exceptionally well compared to others in the portfolio. For now, these algorithms constitute signal processing algorithms such as a Fourier Transform algorithm that attempts to identify peaks in numerical values that would otherwise not be apparent when examined in isolation and without further processing. Therefore, these algorithms will be used to fine-tune the capital allocation percentages for each company in the pre-final portfolio.\n",
    "\n",
    "#### Columns of relevance\n",
    "Data set 1: Overview\n",
    "- Price\n",
    "- MKT Cap\n",
    "- P/E\n",
    "- EPS\n",
    "- Sector\n",
    "\n",
    "Data set 2: Performance\n",
    "- 1M change (1 month change)\n",
    "- 3-Month performance\n",
    "- 6-month perfromance\n",
    "- YTD performance\n",
    "- Yearly performance\n",
    "- Volatility\n",
    "\n",
    "Data set 3: Valuation\n",
    "- Price / revenue\n",
    "- Enterprise value\n",
    "\n",
    "Data set 4: Dividends\n",
    "- Dividend yield FWD\n",
    "- Dividends per share (FY)\n",
    "\n",
    "Data set 5: Margins\n",
    "- Gross profit margin\n",
    "- Operating margin\n",
    "- Net profit margin\n",
    "\n",
    "Data set 6: Income Statement\n",
    "- Gross profit\n",
    "- Income\n",
    "- Net cash flow\n",
    "\n",
    "Data set 7: Balance Sheet\n",
    "- Current ratio\n",
    "- Debt/equity\n",
    "- Quick ratio\n",
    "\n",
    "The total number of columns would be 24 in this case.\n",
    "\n",
    "[^1]: This would mean that a few time series data sets would need to be downloaded from TradingView at the end of the project to test the demo porfolio.\n",
    "\n",
    "[^2]: A helper function can be developed for this, where the user can just type out the name of the company and the ticker is identifed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import matplotlib.pyplot as mplt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import preprocessing\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import plotly.figure_factory as ff\n",
    "from IPython.display import display, HTML, Markdown, Latex\n",
    "from tqdm import tqdm, trange\n",
    "from typing import *\n",
    "from dataclasses import dataclass\n",
    "from scipy import stats\n",
    "import plotly.io as pio\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from analysis.code import project_functions1 as pf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(style=\"darkgrid\")\n",
    "sns.set(rc={\"figure.dpi\":300, 'savefig.dpi':300})\n",
    "sns.set(style=\"ticks\", context=\"talk\")\n",
    "mplt.style.use(\"dark_background\")\n",
    "\n",
    "config = {\n",
    "  'toImageButtonOptions': {\n",
    "    'format': 'png',\n",
    "    'filename': 'custom_image',\n",
    "    'height': 800,\n",
    "    'width': 2000,\n",
    "    'scale': 2\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ValueRange:\n",
    "    min: float\n",
    "    max: float\n",
    "    \n",
    "    def validate(self, x):\n",
    "        \"\"\"Checks if inputs to variables that must lie within a specific range are valid\n",
    "        \n",
    "        :x: the value that must be checked as satisfying the specified range\n",
    "        :raises ValueError: if the value does not lie within the specified range\n",
    "        \"\"\"\n",
    "        if not (self.min <= x <= self.max):\n",
    "            raise ValueError(f'{x} must be between 0 and 1 (including).')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "equities = pf.EquityData()\n",
    "overview_df = equities.load_and_process(\"overview\", exclude_columns=['Change %', 'Change', 'Technical Rating', 'Volume', 'Volume*Price'])\n",
    "income_statement_df = equities.load_and_process(\"income_statement\")\n",
    "balance_sheet_df = equities.load_and_process(\"balance_sheet\")\n",
    "dividends_df = equities.load_and_process(\"dividends\", exclude_columns=['Price'])\n",
    "margins_df = equities.load_and_process(\"margins\")\n",
    "performance_df = equities.load_and_process(\"performance\", exclude_columns=['Change 1m, %', 'Change 5m, %', 'Change 15m, %', 'Change 1h, %', 'Change 4h, %', 'Change 1W, %', 'Change 1M, %', 'Change %'])\n",
    "valuation_df = equities.load_and_process(\"valuation\", exclude_columns=['Price', 'Market Capitalization', 'Price to Earnings Ratio (TTM)', 'Basic EPS (TTM)', 'EPS Diluted (FY)'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis-Specific Data Wrangling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = [\n",
    "    overview_df,\n",
    "    income_statement_df,\n",
    "    balance_sheet_df,\n",
    "    dividends_df,\n",
    "    margins_df,\n",
    "    performance_df,\n",
    "    valuation_df\n",
    "    ]\n",
    "\n",
    "dfs_names = [\n",
    "    \"Overview Data\",\n",
    "    \"Balance Sheet Data\",\n",
    "    \"Dividends Data\",\n",
    "    \"Income Statement Data\",\n",
    "    \"Margins Data\",\n",
    "    \"Performance Data\",\n",
    "    \"Valuation Data\"\n",
    "    ]\n",
    "\n",
    "overview_df['3-Month Performance'] = performance_df['3-Month Performance']\n",
    "income_statement_df['3-Month Performance'] = performance_df['3-Month Performance']\n",
    "balance_sheet_df['3-Month Performance'] = performance_df['3-Month Performance']\n",
    "dividends_df['3-Month Performance'] = performance_df['3-Month Performance']\n",
    "margins_df['3-Month Performance'] = performance_df['3-Month Performance']\n",
    "valuation_df['3-Month Performance'] = performance_df['3-Month Performance']\n",
    "\n",
    "mega_df = pd.concat(dfs, axis=1)\n",
    "mega_df = mega_df.loc[:,~mega_df.columns.duplicated()].copy()\n",
    "mega_df = mega_df.dropna()\n",
    "mega_df_no_strings = mega_df.select_dtypes(exclude='object')\n",
    "\n",
    "mega_df['6-Month Performance'] = performance_df['6-Month Performance']\n",
    "mega_df['YTD Performance'] = performance_df['YTD Performance']\n",
    "mega_df['Yearly Performance'] = performance_df['Yearly Performance']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functional Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantitativeAnalysis:\n",
    "    def __init__(self, number_of_companies: int=500, initial_capital: float=100000.00, capital_per_period: float=100.00, period: int=7, dividends_importance: bool=False, preferred_industries: list=[\"Technology Services, Electronic Technology\"],\n",
    "                volatility_tolerance: Annotated[float, ValueRange(0.0, 1.0)]=0.7, preferred_companies: list=[\"Apple, Google, Microsoft, Amazon\"], diversification: Annotated[float, ValueRange(0.0, 1.0)]=0.4, investment_strategy: str=\"Growth\"):\n",
    "        \"\"\"Includes several analysis functions that process select data across all data sets\n",
    "\n",
    "        :number_of_companies: the number of companies included in the sample, with the default being those from the S&P500 Index\\n\n",
    "        :initial_capital: the initial amount of cash to be invested by the client, in USD\\n\n",
    "        :capital_per_period: the amount of cash to be invested by the client at a fixed rate in addition to the initial capital invested, in USD\\n\n",
    "        :period: the frequency (in days) at which additional cash is invested, if desired\\n\n",
    "        :dividends_importance: specifies whether dividends are important to the client, dictating whether analysis algorithms should place greater importance on dividends\\n\n",
    "        :preferred_industries: specifies a list of industries that the analysis algorithms should prioritize when constructing the investment portfolio\\n\n",
    "        :volatility_tolerance: accepts a range of values from 0 to 1, with 1 implying maximum volatility tolerance (i.e. the client is willing to lose 100% of their investment to take on more risk)\\n\n",
    "        :preferred_companies: specifies a list of companies that the analysis algorithms will accomodate in the final portfolio irrespective of their score\\n\n",
    "        :diversification: accepts a range of values from 0 to 1, with 1 implying maximum diversification (i.e. funds will be distributed evenly across all industries and equally among all companies)\\n\n",
    "        :investment_strategy: specifies the investment strategy that will guide the output of the analysis algorithms, in which this analysis notebook strictly focuses on growth investing\\n\n",
    "        :raises: ValueError if an input parameter does not satisfy its accepted range\n",
    "        \"\"\"\n",
    "        \n",
    "        self.number_of_companies = number_of_companies\n",
    "        self.initial_capital = initial_capital\n",
    "        self.capital_per_period = capital_per_period\n",
    "        self.period = period\n",
    "        self.dividends_importance = dividends_importance\n",
    "        self.preferred_industries = preferred_industries\n",
    "        self.volatility_tolerance = volatility_tolerance\n",
    "        self.preferred_companies = preferred_companies\n",
    "        self.diversification = diversification\n",
    "        self.preferred_companies = preferred_industries\n",
    "        self.investment_strategy = investment_strategy\n",
    "        \n",
    "    def lin_reg_coef_determination(self, df: pd.DataFrame, X: str, y: str='3-Month Performance', filter_outliers: bool=True) -> np.float64:\n",
    "        if filter_outliers:\n",
    "            df = self.outlier_filtered_df(df, col=y)\n",
    "        \n",
    "        X = df[X]\n",
    "        y = df[y]\n",
    "        \n",
    "        y = y.dropna()\n",
    "        X = X.dropna()\n",
    "        \n",
    "        if len(X) > len(y):\n",
    "            X = X[:len(y)]\n",
    "        else:\n",
    "            y = y[:len(X)]\n",
    "        \n",
    "        self.X = np.array(X).reshape(-1, 1)\n",
    "        self.y = np.array(y).reshape(-1, 1)\n",
    "        \n",
    "        model = LinearRegression()\n",
    "        model.fit(self.X, self.y)\n",
    "         \n",
    "        return model.score(self.X, self.y)\n",
    "\n",
    "    def get_lin_reg_coefs(self, df: pd.DataFrame, x_values: list(), y_value: str='3-Month Performance') -> pd.DataFrame:\n",
    "        \"\"\"Returns a Pandas DataFrame with the coefficients of determination for each y-on-x regression\n",
    "        Example: 3-Month Performance against Price to Earnings Ratio (TTM)\n",
    "        \n",
    "        :df: the data frame that contains the columns to process\\n\n",
    "        :x_values: a list of strings of the names of each column to process\\n\n",
    "        :y_value: a common y-value to map each x value against in the regression analysis\\n\n",
    "        :returns: A Pandas DataFrame with the coefficients of determination for each y-on-x regression\\n\n",
    "        \n",
    "        \"\"\"\n",
    "        self.coef_dict = dict.fromkeys(x_values, 0) # initialize a dict with all the columns assigned to a value of 0\n",
    "        \n",
    "        for predictor in tqdm(x_values, desc=\"Constructing linear regression models\", total=len(x_values)):\n",
    "            self.coef_dict[predictor] = self.lin_reg_coef_determination(df, X=predictor, y=y_value)\n",
    "        \n",
    "        self.processed_df = pd.DataFrame(list(zip(self.coef_dict.keys(), self.coef_dict.values())), columns=[f'Equity Data Against {y_value}', 'Coefficient of Determination'])\n",
    "        \n",
    "        return self.processed_df\n",
    "        \n",
    "    def fourier_transform(self):\n",
    "        pass\n",
    "    \n",
    "    def rank(self, df: pd.DataFrame, col: str, normalize_only: bool=True, threshold: float=1.5,\n",
    "             below_threshold: bool=True, filter_outliers: bool=True, normalize_after: bool=False,\n",
    "             lower_quantile: float=0.05, upper_quantile: float=0.95) -> None:\n",
    "        \"\"\"The scoring algorithm for determining the weight of each equity in the construction of the portfolio for this specific column examined.\n",
    "        Features a custom outlier-filtering algorithm that is robust to outliers in the data set while still returning normalized values.\n",
    "        \n",
    "        :df: The original dataframe\\n\n",
    "        :col: The name of the column being extracted from the dataframe provided\\n\n",
    "        :normalize_only: if True, does not apply a threshold to the screening algorithm, and only normalizes values with a minmax scaler\\n\n",
    "        :threshold: the minimum value that equities must have for that column in order to be considered for further analysis\\n\n",
    "        :below_threshold: if True, removes equities that are below the threshold for that column\\n\n",
    "        :filter_outliers: if True, does not consider equities in the data normalization algorithm, but assigns a min or max value to all outliers depending on the below_threshold parameter\\n\n",
    "        :normalize_after: if True, normalizes the data only after the threshold filter has been applied\\n\n",
    "        :lower_quantile: specifies the lower quantile of the distribution when filtering outliers\\n\n",
    "        :upper_quantile: specifies the upper quantile of the distribution when filtering outliers\\n\n",
    "        \"\"\"\n",
    "        \n",
    "        #NOTE: should make an option for no threshold\n",
    "        self.x = df[col]\n",
    "        new_col = col + \" Score\"\n",
    "        \n",
    "        # normalization can be done either before or after equities have been filtered by the threshold\n",
    "        # the difference is that by filtering initially, the min and max values of that smaller set will become 0 and 1 respectively\n",
    "        df[new_col] = np.NaN # initialize the score column with only NaN values\n",
    "        \n",
    "        def outlier_filter(self):\n",
    "            \"\"\"Nested helper function to filter outliers\"\"\"\n",
    "            upper_fence = self.x.quantile(upper_quantile)\n",
    "            lower_fence = self.x.quantile(lower_quantile)\n",
    "            \n",
    "            if below_threshold:\n",
    "                df.loc[self.x > upper_fence, new_col] = 1 # outliers still need to be included in the data (max score assigned)\n",
    "                df.loc[self.x < lower_fence, new_col] = 0 # lowest score assigned\n",
    "            else:\n",
    "                # inverse of the above\n",
    "                df.loc[self.x > upper_fence, new_col] = 0\n",
    "                df.loc[self.x < lower_fence, new_col] = 1\n",
    "\n",
    "            # now only take the rows that are not outliers into the minmax scaler\n",
    "            self.x = self.x[(self.x <= upper_fence) & (self.x >= lower_fence)]\n",
    "            \n",
    "            if normalize_only:\n",
    "                normalize_after = False\n",
    "                \n",
    "            if normalize_after:\n",
    "                if below_threshold:\n",
    "                    # since we are only taking valid values, we consider the inverse of the values that are below the threshold to be valid values\n",
    "                    self.x = self.x[self.x >= threshold]\n",
    "                else:\n",
    "                    self.x = self.x[self.x <= threshold]\n",
    "        \n",
    "        if filter_outliers:\n",
    "            outlier_filter(self)\n",
    "        \n",
    "        self.y = np.array(self.x).reshape(-1, 1)\n",
    "        self.y = preprocessing.MinMaxScaler().fit_transform(self.y)\n",
    " \n",
    "        for col_idx, array_idx in zip(self.x.index, range(len(self.y))):\n",
    "            df.at[col_idx, new_col] = self.y[array_idx]\n",
    "        \n",
    "        # if we are giving the minimum score to values below the threshold, assign 0 to those values\n",
    "        if not normalize_only:\n",
    "            if below_threshold:\n",
    "                df.loc[df[col] <= threshold, new_col] = 0\n",
    "            else:\n",
    "                df.loc[df[col] >= threshold, new_col] = 0\n",
    "    \n",
    "    def outlier_filtered_df(self, df: pd.DataFrame, col: list(), lower_quantile: float=0.05, upper_quantile: float=0.95):\n",
    "        upper_fence = df[col].quantile(upper_quantile)\n",
    "        lower_fence = df[col].quantile(lower_quantile)\n",
    "\n",
    "        df = df[(df[col] <= upper_fence) & (df[col] >= lower_fence)]\n",
    "        \n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataVisualization(QuantitativeAnalysis):\n",
    "    def __init__(self):\n",
    "        QuantitativeAnalysis.__init__(self)\n",
    "\n",
    "    def score_density_plot(self, df: pd.DataFrame, data_name: str) -> plt.graph_objs._figure.Figure:\n",
    "        \"\"\"Constructs an interactive compound density plot based on a histogram of the data provided, plotting a density curve with clusters of data points below\n",
    "        \n",
    "        :df: a Pandas DataFrame of equity data\n",
    "        :data_name: the name of the type of data that has been input into the plot\n",
    "        :returns: a density plot\n",
    "        \"\"\"\n",
    "        df = df.select_dtypes(exclude='object')[:self.number_of_companies]\n",
    "        self.n = len(df)\n",
    "        \n",
    "        for column in df.columns:\n",
    "            self.rank(df, col=column, upper_quantile=0.99, lower_quantile=0.01)\n",
    "            \n",
    "        self.score_data_length = len(df.axes[1])\n",
    "        self.input_df = df.T[int(self.score_data_length/2 + 1):].T\n",
    "        self.hist_data = [self.input_df[x] for x in self.input_df.columns]\n",
    "        \n",
    "        self.group_labels = [x for x in self.input_df.columns]\n",
    "        self.colors = ['#333F44', '#37AA9C', '#94F3E4']\n",
    "\n",
    "        self.fig = ff.create_distplot(self.hist_data, self.group_labels, show_hist=False, colors=self.colors)\n",
    "        self.fig.update_layout(title_text=f'Distribution for Normalized {data_name} of {self.n} Companies in the S&P500', template='plotly_dark')\n",
    "        \n",
    "        self.fig.update_xaxes(title='Score (0 = low, 1 = high)')\n",
    "        self.fig.update_yaxes(title='Density')\n",
    "        \n",
    "        return self.fig\n",
    "\n",
    "    def heatmap_plot(self, df: pd.DataFrame, data_name: str, number_of_companies: int=50, correlation_plot: bool=False) -> plt.graph_objs._figure.Figure:\n",
    "        \"\"\"Constructs an interactive heatmap plot of equity data against each company (ticker)\n",
    "        \n",
    "        :df: a Pandas DataFrame of equity data\n",
    "        :data_name: the name of the type of data that has been input into the plot\n",
    "        :number_of_companies: the number of companies to include in the heatmap\n",
    "        :correlation_plot: if true, creates a correlation plot instead of a heatmap plot\n",
    "        :returns: a heatmap plot\n",
    "        \"\"\"\n",
    "        def construct_correlation_plot(self) -> pd.DataFrame:\n",
    "            \"\"\"A helper function to convert the heat map into a correlation plot\"\"\"\n",
    "            # Correlation\n",
    "            self.df_corr = df.corr(numeric_only=True).round(1)\n",
    "            # Conver to a triangular correlation plot\n",
    "            self.mask = np.zeros_like(self.df_corr, dtype=bool)\n",
    "            self.mask[np.triu_indices_from(self.mask)] = True\n",
    "            # Final visualization\n",
    "            self.df_corr_viz = self.df_corr.mask(self.mask).dropna(how='all').dropna('columns', how='all')\n",
    "            \n",
    "            return self.df_corr_viz\n",
    "    \n",
    "        if correlation_plot:            \n",
    "            data_points = len(df.columns)\n",
    "            self.title = f'Correlation Plot of {data_points} Data Points of {data_name}'\n",
    "            \n",
    "            self.cor_df = construct_correlation_plot(self)\n",
    "            self.fig = px.imshow(\n",
    "                self.cor_df,\n",
    "                text_auto=True,\n",
    "                template='plotly_dark',\n",
    "                title=self.title,\n",
    "                width=1000,\n",
    "                height=1000)\n",
    "        else:\n",
    "            df = df[:number_of_companies]\n",
    "            self.z = []\n",
    "            self.tickers = df['Ticker']\n",
    "            df.index = df['Ticker']\n",
    "            df = df.select_dtypes(exclude='object')\n",
    "            for column in df.columns:\n",
    "                self.rank(df, col=column)\n",
    "\n",
    "            self.score_data_length = len(df.axes[1])\n",
    "            self.input_df = df.T[int(self.score_data_length/2 + 1):].T\n",
    "            for column in self.input_df.columns:\n",
    "                self.z.append(self.input_df[column].round(3))\n",
    "            \n",
    "            self.title = f'Heat Map of Normalized {data_name} for the Top {number_of_companies} Companies by Market Capitalization in the S&P500 Index'\n",
    "            \n",
    "            self.fig = px.imshow(\n",
    "                self.z,\n",
    "                text_auto=True,\n",
    "                template='plotly_dark',\n",
    "                title=self.title,\n",
    "                x=[x for x in self.tickers], \n",
    "                y=[x for x in df.columns[int(self.score_data_length/2 + 1):]])\n",
    "        \n",
    "        return self.fig\n",
    "\n",
    "    def scatter_3d(self, df: pd.DataFrame, x: str, y: str, z: str) -> plt.graph_objs._figure.Figure:\n",
    "        \"\"\"Constructs a 3D interactive plot of equity data on 3 axes\n",
    "        :df: a Pandas DataFrame of equity data\n",
    "        :x: the name of the column data to be plotted on the x-axis, as a string\n",
    "        :y: the name of the column data to be plotted on the y-axis, as a string\n",
    "        :z: the name of the column data to be plotted on the z-axis, as a string\n",
    "        :returns: a 3D scatter plot\n",
    "        \"\"\"\n",
    "        df.index = df['Ticker']\n",
    "        df = df.select_dtypes(exclude='object')\n",
    "        \n",
    "        for column in df.columns:\n",
    "            self.rank(df, col=column)\n",
    "            \n",
    "        fig = px.scatter_3d(df, x=x, y=y, z=z,\n",
    "                    title='3D Scatter Plot of Normalized Equity Data',\n",
    "                    template='plotly_dark',\n",
    "                    size_max=18,\n",
    "                    color='3-Month Performance Score',\n",
    "                    opacity=0.7)\n",
    "\n",
    "        return fig\n",
    "    \n",
    "    def correlation_plot(self, df, data_name) -> None: \n",
    "        \"\"\"Produces a correlation plot that maps all of the data points in the Data Frame provided\n",
    "        :df: a Pandas DataFrame of the data to be processed\n",
    "        :data_name: the name of the data being plotted\n",
    "        \"\"\"\n",
    "        # Compute the correlation matrix\n",
    "        self.corr = df.corr(numeric_only=True)\n",
    "\n",
    "        # Generate a mask for the upper triangle\n",
    "        self.mask = np.triu(np.ones_like(self.corr, dtype=bool))\n",
    "\n",
    "        # Set up the matplotlib figure\n",
    "        self.f, self.ax = plt.subplots(figsize=(18, 14))\n",
    "\n",
    "        # Generate a custom diverging colormap\n",
    "        self.cmap = sns.diverging_palette(1, 10, as_cmap=True)\n",
    "\n",
    "        #Draw the heatmap with the mask and correct aspect ratio\n",
    "        sns.heatmap(self.corr, mask=self.mask, cmap=self.cmap, vmax=.3, center=0,\n",
    "                    square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n",
    "        mplt.title(f\"Correlation Plot of {data_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PortfolioConstruction(DataVisualization):\n",
    "    def __init__(self):\n",
    "        DataVisualization.__init__(self)\n",
    "    \n",
    "    def asset_allocation(self):\n",
    "        pass\n",
    "    \n",
    "    def construct_portfolio(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quant = QuantitativeAnalysis()\n",
    "viz = DataVisualization()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimental Feature Development Zone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis Zone\n",
    "## Note: to view the interactive graphs plotted, run this analysis notebook in a Jupyter Notebook environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#viz.correlation_plot(mega_df, 'S&P 500 Equity Data')\n",
    "viz.heatmap_plot(mega_df, 'S&P500 Equity Data', number_of_companies=500, correlation_plot=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Values with a correlation coefficient greater than or equal to 0.7 are considered as strong correlations. Likewise, negative correlation coefficients follow the inverse of this criteria. The purpose of this correlation plot is to identify singular variables that are correlated with the positive returns of many other variables that are often considered as benchmarks for strong equity performance. Even more significant is the identification of variables that normally do not have any apparent correlation when viewed in isolation, but do when paired together in a regression. After establishing such occurrences, certain variables can be assigned a stronger weight than others when being processed by the normalization algorithm that is the basis of our equity ranking system used in our asset allocation and construction algorithm. Such variables can later be grouped as predictors into a multiple linear regression model for further analysis. The results from the correlation plot can be classified as follows:\n",
    "\n",
    "| X-Value(s) | Strong Positive Y-Values (r >= 0.7) |\n",
    "| --- | --- |\n",
    "| Market Capitalization | Total Shares Outstanding, Net Income (FY), Gross Profit (FY), Gross Profit (MRQ), EBITDA (TTM), Total Current Assets (MRQ) |\n",
    "| Basic EPS (TTM) | EPS Diluted (FY), EPS Diluted (TTM), Basic EPS FY |\n",
    "| EBITDA (TTM), Gross Profit (MRQ), Gross Profit (FY) | Total Shares Outstanding, Enterprise Value (MRQ), Total Current Assets (MRQ), Total Assets (MRQ), Net Income (FY), Gross Profit (FY), Gross Profit (MRQ), Total Debt (MRQ), Last Year Revenue (FY), Total Revenue (FY)|\n",
    "| Total Revenue (FY) | Total Current Assets (MRQ), Last Year Revenue (FY), Total Assets (MRQ) |\n",
    "| Last Year Revenue (FY) | Total Current Assets (MRQ), Total Assets (MRQ) |\n",
    "| Current Ratio (MRQ) | Quick ratio (MRQ), Total Shares Outstanding, Enterprise Value (MRQ) |\n",
    "| Total Assets (MRQ) | Total Current Assets (MRQ), Total Debt (MRQ) |\n",
    "| Operating Margin (TTM) | Net Margin (TTM), Pretax Margin (TTM) |\n",
    "| Enterprise Value (MRQ) | Total Shares Outstanding |\n",
    "| Number of Employees | Last Year Revenue, Total Revenue (FY) |\n",
    "| Net Income | Total Shares Outstanding, Enterprise Value, Total Current Assets (MRQ), Total Assets (MRQ) |\n",
    "| Net Debt | Total Assets (MRQ) |\n",
    "| Gross Margin (TTM) | Price to Revenue Ratio (TTM), Net Margin (TTM), Pretax Margin (TTM), Operating Margin (TTM) |\n",
    "| Price to Revenue Ratio (TTM) | Enterprise Value / EBITDA (TTM) |\n",
    "\n",
    "It should be noted, however, that many of the X values show a high correlation with other Y values due to those Y values being a derivative of the initial X value and vice versa. This observation confirms that  Taking this feature into account can give the following results:\n",
    "\n",
    "| X-Value(s) | Strong Positive Y-Values (r >= 0.7) |\n",
    "| --- | --- |\n",
    "| Market Capitalization | Total Shares Outstanding, Net Income (FY), Gross Profit (FY), Gross Profit (MRQ), EBITDA (TTM), Total Current Assets (MRQ) |\n",
    "| EBITDA (TTM), Gross Profit (MRQ), Gross Profit (FY) | Total Shares Outstanding, Enterprise Value (MRQ), Total Current Assets (MRQ), Total Assets (MRQ), Net Income (FY), Gross Profit (FY), Gross Profit (MRQ), Total Debt (MRQ), Last Year Revenue (FY), Total Revenue (FY) |\n",
    "| Current Ratio (MRQ) | Quick ratio (MRQ), Total Shares Outstanding, Enterprise Value (MRQ) |\n",
    "| Enterprise Value (MRQ) | Total Shares Outstanding |\n",
    "| Number of Employees | Last Year Revenue, Total Revenue (FY) |\n",
    "| Net Income | Total Shares Outstanding, Enterprise Value, Total Current Assets (MRQ), Total Assets (MRQ) |\n",
    "| Price to Revenue Ratio (TTM) | Enterprise Value / EBITDA (TTM) |\n",
    "\n",
    "Taking variables that may not have any immediate obvious correlation can yield the following:\n",
    "\n",
    "| X-Value(s) | Strong Positive Y-Values (r >= 0.7) |\n",
    "| --- | --- |\n",
    "| EBITDA (TTM), Gross Profit (MRQ), Gross Profit (FY) | Total Shares Outstanding |\n",
    "| Current Ratio (MRQ) | Total Shares Outstanding |\n",
    "| Enterprise Value (MRQ) | Total Shares Outstanding |\n",
    "| Number of Employees | Last Year Revenue, Total Revenue (FY) |\n",
    "| Net Income | Total Shares Outstanding |\n",
    "\n",
    "An outcome of these observations can be to take the correlations of these variables and assign them as a multiplier to the normalized values for each respective column to prioritize certain equity data as being more deterministic of a positive return on investment than others.\n",
    "\n",
    "Another outcome of these observations can be to analyze the density plot creating during the EDA phase and solely focus on the distributions of the above variables from tables 2 or 3 that are skewed towads low normalized values. The logic with this would be that outliers in negatively skewed distributions are more likely to be indicative of stronger performance because they excel in a financial ratio that very few companies excel in.\n",
    "\n",
    "The weighted scores from each analysis can be aggregated and in the end, a weighted scoring system is used."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
