{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Colin Lefter"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Research question/interests\n",
    "\n",
    "**What equity data is the most deterministic of the performance of an equity, and of this data, which is the most relevant for a growth portfolio investment strategy such that we can compute an optimized portfolio of equities while using user input to drive our optimization algorithm?**\n",
    "\n",
    "My research objective is to develop a scalable asset allocation and construction algorithm that implements an objected-oriented design approach. This objective is an outcome of determining what equity data is the most deterministic of the price of an equity, which will be the focus for the majority of the project/\n",
    "\n",
    "I intend to develop algorithms for constructing multiple linear regressions and Fourier Franforms, among others, that I will then use to construct interactive and statistical models with Plotly and Seaborn. As such, I have a strong interest in the system design of our software and in developing helper functions that can assist all of us with processing data more efficiently. I am also looking forward to using Facebook Prophet[^1] to construct a time series forecast of a sample portfolio recommendation from our software, which can be included in our Tableau Dashboard.\n",
    "\n",
    "### Analysis Plan\n",
    "Our objective function is one that takes in a selection of columns from our data sets to then search for the top n companies that satisfy a criteria for having the highest probability of producing an optimal return on investment. These inputs themselves refer to sub-objective functions that take as input user-defined parameters and thresholds that set the criteria for favourable performance attributes. To rank the companies from our data set, and ultimately determine what portion of capital to assign to each equity, I propose a data normalization algorithm that normalizes the data that comprises the favourable subset from each column of our data set. We interpret these normalized values as probabilities of equity selection and ultimately average the score of each company across all columns to then multiply the final score percentage of each company with the total capital specified by the user. In a broad sense, our software is composed of four general classes that include \"Data\", \"Quantitative Analysis\", \"Data Visualization\" and \"Portfolio Construction\". We inherit the properties from each of these classes to build a functional data analysis chain.\n",
    "\n",
    "Our data visualization will be concerned with analyzing the influence of certain financial variables, such as Price-to-Earnings, on the price of each equity from a sample of 500 equities (from the S&P 500 index). Such analysis would begin with a statistical summary that will constitute exploratory data analysis, followed by our application of analysis algorithms that we design. The construction of a portfolio is a bonus of our project and will be made possible by the analysis algorithms we have constructed.\n",
    "\n",
    "**Important Note**\n",
    "A component of the analysis will involve the comparison of different values of financial variables with the corresponding price of each equity. This constitutes inferential analysis as we are attempting to identify a correlation on the basis of picking stocks based on expected performance. Therefore, this will require us to use past financial data and compare this data with the current price of each equity. As a result, we can only use the 3-month performance data (i.e. 3-month change in share price data) for this comparison as otherwise we would be using future data to predict past performance, which would be invalid.\n",
    "\n",
    "#### User-defined parameters\n",
    "Some initial ideas for these parameters include:\n",
    "- (float) Initial capital\n",
    "- (float) Additional capital per day, week or month\n",
    "- (int) Intended holding period (in days)\n",
    "- (boolean) Importance of dividends (validated based on capital invested)\n",
    "- (String) Preferred industries (choose from a list, or select all)\n",
    "- (int) Volatility tolerance (from 0 to 1, 1 indicating that volatility is not important)\n",
    "- (String) Preferred companies (as a list)[^2]\n",
    "- (int) Preferred degree of portfolio diversification (from 0 to 1, 1 indicating complete diversification)\n",
    "- (String) Preferred investment strategy (choose from \"Growth\", \"Value\", \"GARP\")\n",
    "\n",
    "### Algorithm Plan\n",
    "\n",
    "####  Tier 1: Threshold-based screening algorithms\n",
    "- The current plan is to use these algorithms to screen the financial documents from each company by setting a minimum threshold for each financial ratio. This class of algorithms will need to conduct such screening per industry as industry financial ratios are dinstinct from one another.\n",
    "- A global screening algorithm that selects companies which show favourable performance across all ratios can also be used after each ratio has been individually tested.\n",
    "\n",
    "#### Tier 2: Regression models\n",
    "- As of now, the intent is to develop a multiple linear regression model that will attempt to determine a relationship between the yearly and quarterly performance of each company in relation to several columns of data that act as predictors. This can essentially implement the results from the threshold-based screening algorithms to only conduct this analysis on the pre-screened companies.\n",
    "\n",
    "#### Tier 3: Statistical modelling algorithms\n",
    "- Tier 3 denotes a class of broadly experimental statistical modelling algorithms that are applied on a pre-final portfolio to add additional points to companies that perform exceptionally well compared to others in the portfolio. For now, these algorithms constitute signal processing algorithms such as a Fourier Transform algorithm that attempts to identify peaks in numerical values that would otherwise not be apparent when examined in isolation and without further processing. Therefore, these algorithms will be used to fine-tune the capital allocation percentages for each company in the pre-final portfolio.\n",
    "\n",
    "#### Columns of relevance\n",
    "Data set 1: Overview\n",
    "- Price\n",
    "- MKT Cap\n",
    "- P/E\n",
    "- EPS\n",
    "- Sector\n",
    "\n",
    "Data set 2: Performance\n",
    "- 1M change (1 month change)\n",
    "- 3-Month performance\n",
    "- 6-month perfromance\n",
    "- YTD performance\n",
    "- Yearly performance\n",
    "- Volatility\n",
    "\n",
    "Data set 3: Valuation\n",
    "- Price / revenue\n",
    "- Enterprise value\n",
    "\n",
    "Data set 4: Dividends\n",
    "- Dividend yield FWD\n",
    "- Dividends per share (FY)\n",
    "\n",
    "Data set 5: Margins\n",
    "- Gross profit margin\n",
    "- Operating margin\n",
    "- Net profit margin\n",
    "\n",
    "Data set 6: Income Statement\n",
    "- Gross profit\n",
    "- Income\n",
    "- Net cash flow\n",
    "\n",
    "Data set 7: Balance Sheet\n",
    "- Current ratio\n",
    "- Debt/equity\n",
    "- Quick ratio\n",
    "\n",
    "The total number of columns would be 24 in this case.\n",
    "\n",
    "[^1]: This would mean that a few time series data sets would need to be downloaded from TradingView at the end of the project to test the demo porfolio.\n",
    "\n",
    "[^2]: A helper function can be developed for this, where the user can just type out the name of the company and the ticker is identifed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as mplt\n",
    "import plotly.graph_objects as go\n",
    "from IPython.display import display, HTML, Markdown, Latex\n",
    "import sys\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import warnings # remove later\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning) # remove later\n",
    "\n",
    "sys.path.append('..')\n",
    "from analysis.code import project_functions1 as pf"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(style=\"darkgrid\")\n",
    "sns.set(rc={\"figure.dpi\":300, 'savefig.dpi':300})\n",
    "sns.set(style=\"ticks\", context=\"talk\")\n",
    "mplt.style.use(\"dark_background\")\n",
    "\n",
    "config = {\n",
    "  'toImageButtonOptions': {\n",
    "    'format': 'png',\n",
    "    'filename': 'custom_image',\n",
    "    'height': 3500,\n",
    "    'width': 2000,\n",
    "    'scale': 2\n",
    "  }\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "equities = pf.EquityData()\n",
    "performance_df = equities.load_and_process(\"performance\", exclude_columns=['Change 1m, %', 'Change 5m, %', 'Change 15m, %', 'Change 1h, %', 'Change 4h, %', 'Change 1W, %', 'Change 1M, %', 'Change %'])\n",
    "overview_df = equities.load_and_process(\"overview\", exclude_columns=['Change %', 'Change', 'Technical Rating', 'Volume', 'Volume*Price'], additional_data=performance_df, additional_column='3-Month Performance')\n",
    "income_statement_df = equities.load_and_process(\"income_statement\", additional_data=performance_df, additional_column='3-Month Performance')\n",
    "balance_sheet_df = equities.load_and_process(\"balance_sheet\", additional_data=performance_df, additional_column='3-Month Performance')\n",
    "dividends_df = equities.load_and_process(\"dividends\", exclude_columns=['Price'], additional_data=performance_df, additional_column='3-Month Performance')\n",
    "margins_df = equities.load_and_process(\"margins\", additional_data=performance_df, additional_column='3-Month Performance')\n",
    "valuation_df = equities.load_and_process(\"valuation\", exclude_columns=['Price', 'Market Capitalization', 'Price to Earnings Ratio (TTM)', 'Basic EPS (TTM)', 'EPS Diluted (FY)'], additional_data=performance_df, additional_column='3-Month Performance')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis-Specific Data Wrangling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = [\n",
    "    overview_df,\n",
    "    income_statement_df,\n",
    "    balance_sheet_df,\n",
    "    dividends_df,\n",
    "    margins_df,\n",
    "    performance_df,\n",
    "    valuation_df\n",
    "    ]\n",
    "\n",
    "dfs_names = [\n",
    "    \"Overview Data\",\n",
    "    \"Balance Sheet Data\",\n",
    "    \"Dividends Data\",\n",
    "    \"Income Statement Data\",\n",
    "    \"Margins Data\",\n",
    "    \"Performance Data\",\n",
    "    \"Valuation Data\"\n",
    "    ]\n",
    "\n",
    "mega_df = equities.combined_data_frame(dfs)\n",
    "mega_df_no_strings = mega_df.select_dtypes(exclude='object')\n",
    "mega_df['6-Month Performance'] = performance_df['6-Month Performance']\n",
    "mega_df['YTD Performance'] = performance_df['YTD Performance']\n",
    "mega_df['Yearly Performance'] = performance_df['Yearly Performance']\n",
    "mega_df['Quick Ratio (MRQ)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quant = pf.QuantitativeAnalysis()\n",
    "viz = pf.DataVisualization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "equities_temp = pf.EquityData(\"processed_us_equities_tradingview_data_\")\n",
    "temp = equities_temp.load_and_process('complete_data', directory_path=\"../data/processed/\")\n",
    "mega_df = temp\n",
    "original_temp = temp\n",
    "previous_cols_with_str = temp.columns\n",
    "str_cols_only = temp.select_dtypes(include='object')\n",
    "temp = temp.select_dtypes(exclude='object')\n",
    "\n",
    "previous_cols_no_str = temp.columns\n",
    "\n",
    "for col in temp.columns:\n",
    "    quant.rank(temp, col)\n",
    "\n",
    "temp = temp[[col + ' Score' for col in previous_cols_no_str]]\n",
    "for col in str_cols_only:\n",
    "    temp[col] = original_temp[col]\n",
    "\n",
    "#equities.save_processed_data([temp], ['normalized_data'])\n",
    "#temp"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimental Feature Development Zone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notable_significance = [\n",
    "    'EPS Diluted (TTM)',\n",
    "    'Price to Earnings Ratio (TTM)',\n",
    "    'Debt to Equity Ratio (MRQ)',\n",
    "    'Enterprise Value/EBITDA (TTM)',\n",
    "    'Operating Margin (TTM)',\n",
    "    'Quick Ratio (MRQ)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#px.bar(x=score_count_df.columns, y=score_count_df.T['Count'])\n",
    "\n",
    "score_count_df = quant.extract_corr_plot_counts(mega_df).T\n",
    "max_count = max(score_count_df['Count'])\n",
    "\n",
    "score_count_df['Assigned Weight'] = score_count_df['Count'] / sum(score_count_df['Count'])\n",
    "\n",
    "top_predictors = score_count_df[score_count_df['Count'] >= 4]\n",
    "top_predictors_wide = score_count_df[score_count_df['Count'] >= 4].index # 4 x 4 grid\n",
    "top_predictors_narrow = score_count_df[score_count_df['Count'] >= 5].index\n",
    "top_predictors_narrowest = score_count_df[score_count_df['Count'] >= 6].index\n",
    "top_predictors_narrowest_adjusted = top_predictors_narrowest.drop(['Gross Profit (FY)', 'Enterprise Value (MRQ)'])\n",
    "# Gross Profit (FY) and MRQ are the same, so FY is removed.\n",
    "# Enterprise Value (MRQ) subtracts total debt from market capitalization, so it tracks the score of Market Capitalization nearly identically, so it is removed.\n",
    "#top_predictors_narrowest_adjusted\n",
    "\n",
    "score_count_df = score_count_df.sort_values(by='Assigned Weight', ascending=False)\n",
    "quant.rank(score_count_df, 'Assigned Weight', filter_outliers=False)\n",
    "#equities.save_processed_data([score_count_df], ['deterministic_data'])\n",
    "score_count_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "equities_2 = pf.EquityData(\"processed_us_equities_tradingview_data_\")\n",
    "#data_1 = equities_2.load_and_process(\"deterministic_data\", directory_path=\"../data/processed/\")\n",
    "#data_2 = equities_2.load_and_process(\"demo_portfolio\", directory_path=\"../data/processed/\")\n",
    "#tableau_data = pd.concat([data_1, data_2])\n",
    "#tableau_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "portfolio = pf.PortfolioRecommendation(500, initial_capital=500000)\n",
    "demo_portfolio = portfolio.asset_allocation()\n",
    "demo_portfolio = demo_portfolio[['Ticker', 'Aggregated Score']]\n",
    "#equities.save_processed_data([demo_portfolio], ['complete_aggregated_scores'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_data = equities_temp.load_and_process('normalized_data', directory_path=\"../data/processed/\")\n",
    "\n",
    "target_y = 'Market Capitalization Score'\n",
    "predictors = ['EPS Diluted (TTM) Score', 'Price to Earnings Ratio (TTM) Score', 'Debt to Equity Ratio (MRQ) Score', 'Enterprise Value/EBITDA (TTM) Score', 'Operating Margin (TTM) Score', 'Quick Ratio (MRQ) Score']\n",
    "\n",
    "normalized_data = normalized_data.dropna()\n",
    "top_50 = normalized_data[normalized_data['Market Capitalization Score'] > 0.6]\n",
    "\n",
    "quant.multiple_linear_regression(top_50, predictors, target_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(\n",
    "    top_50, x=target_y, y=predictors, opacity=0.65, trendline='ols',\n",
    "    trendline_color_override='darkblue'\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis Zone\n",
    "## Note: to view the interactive graphs plotted, run this analysis notebook in a Jupyter Notebook environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quant.rank(mega_df, 'Price')\n",
    "# quant.rank(mega_df, 'Market Capitalization')\n",
    "# quant.rank(mega_df, 'Basic EPS (TTM)')\n",
    "# mega_df\n",
    "\n",
    "#quant.multiple_linear_regression(mega_df, ['Basic EPS (TTM) Score'], target_y='Price Score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#viz.correlation_plot(mega_df, 'S&P 500 Equity Data')\n",
    "fig = viz.heatmap_plot(mega_df, 'Correlation Plot of S&P500 Equity Data', number_of_companies=500, correlation_plot=True)\n",
    "\n",
    "config = {\n",
    "  'toImageButtonOptions': {\n",
    "    'format': 'png',\n",
    "    'filename': 'custom_image',\n",
    "    'height': 2000,\n",
    "    'width': 2500,\n",
    "  }\n",
    "}\n",
    "\n",
    "fig.show(config=config)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No correlation:\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Values with a correlation coefficient greater than or equal to 0.7 are considered as strong correlations. Likewise, negative correlation coefficients follow the inverse of this criteria. The purpose of this correlation plot is to identify singular variables that are correlated with the positive returns of many other variables that are often considered as benchmarks for strong equity performance. Even more significant is the identification of variables that normally do not have any apparent correlation when viewed in isolation, but do when paired together in a regression. After establishing such occurrences, certain variables can be assigned a stronger weight than others when being processed by the normalization algorithm that is the basis of our equity ranking system used in our asset allocation and construction algorithm. Such variables can later be grouped as predictors into a multiple linear regression model for further analysis. The results from the correlation plot can be classified as follows:\n",
    "\n",
    "| X-Value(s) | Strong Positive Y-Values (r >= 0.7) |\n",
    "| --- | --- |\n",
    "| Market Capitalization | Total Shares Outstanding, Net Income (FY), Gross Profit (FY), Gross Profit (MRQ), EBITDA (TTM), Total Current Assets (MRQ) |\n",
    "| Basic EPS (TTM) | EPS Diluted (FY), EPS Diluted (TTM), Basic EPS FY |\n",
    "| EBITDA (TTM), Gross Profit (MRQ), Gross Profit (FY) | Total Shares Outstanding, Enterprise Value (MRQ), Total Current Assets (MRQ), Total Assets (MRQ), Net Income (FY), Gross Profit (FY), Gross Profit (MRQ), Total Debt (MRQ), Last Year Revenue (FY), Total Revenue (FY)|\n",
    "| Total Revenue (FY) | Total Current Assets (MRQ), Last Year Revenue (FY), Total Assets (MRQ) |\n",
    "| Last Year Revenue (FY) | Total Current Assets (MRQ), Total Assets (MRQ) |\n",
    "| Current Ratio (MRQ) | Quick ratio (MRQ), Total Shares Outstanding, Enterprise Value (MRQ) |\n",
    "| Total Assets (MRQ) | Total Current Assets (MRQ), Total Debt (MRQ) |\n",
    "| Operating Margin (TTM) | Net Margin (TTM), Pretax Margin (TTM) |\n",
    "| Enterprise Value (MRQ) | Total Shares Outstanding |\n",
    "| Number of Employees | Last Year Revenue, Total Revenue (FY) |\n",
    "| Net Income | Total Shares Outstanding, Enterprise Value, Total Current Assets (MRQ), Total Assets (MRQ) |\n",
    "| Net Debt | Total Assets (MRQ) |\n",
    "| Gross Margin (TTM) | Price to Revenue Ratio (TTM), Net Margin (TTM), Pretax Margin (TTM), Operating Margin (TTM) |\n",
    "| Price to Revenue Ratio (TTM) | Enterprise Value / EBITDA (TTM) |\n",
    "\n",
    "It should be noted, however, that many of the X values show a high correlation with other Y values due to those Y values being a derivative of the initial X value and vice versa. This observation confirms that  Taking this feature into account can give the following results:\n",
    "\n",
    "| X-Value(s) | Strong Positive Y-Values (r >= 0.7) |\n",
    "| --- | --- |\n",
    "| Market Capitalization | Total Shares Outstanding, Net Income (FY), Gross Profit (FY), Gross Profit (MRQ), EBITDA (TTM), Total Current Assets (MRQ) |\n",
    "| EBITDA (TTM), Gross Profit (MRQ), Gross Profit (FY) | Total Shares Outstanding, Enterprise Value (MRQ), Total Current Assets (MRQ), Total Assets (MRQ), Net Income (FY), Gross Profit (FY), Gross Profit (MRQ), Total Debt (MRQ), Last Year Revenue (FY), Total Revenue (FY) |\n",
    "| Current Ratio (MRQ) | Quick ratio (MRQ), Total Shares Outstanding, Enterprise Value (MRQ) |\n",
    "| Enterprise Value (MRQ) | Total Shares Outstanding |\n",
    "| Number of Employees | Last Year Revenue, Total Revenue (FY) |\n",
    "| Net Income | Total Shares Outstanding, Enterprise Value, Total Current Assets (MRQ), Total Assets (MRQ) |\n",
    "| Price to Revenue Ratio (TTM) | Enterprise Value / EBITDA (TTM) |\n",
    "\n",
    "Taking variables that may not have any immediate obvious correlation can yield the following:\n",
    "\n",
    "| X-Value(s) | Strong Positive Y-Values (r >= 0.7) |\n",
    "| --- | --- |\n",
    "| EBITDA (TTM), Gross Profit (MRQ), Gross Profit (FY) | Total Shares Outstanding |\n",
    "| Current Ratio (MRQ) | Total Shares Outstanding |\n",
    "| Enterprise Value (MRQ) | Total Shares Outstanding |\n",
    "| Number of Employees | Last Year Revenue, Total Revenue (FY) |\n",
    "| Net Income | Total Shares Outstanding |\n",
    "\n",
    "An outcome of these observations can be to take the correlations of these variables and assign them as a multiplier to the normalized values for each respective column to prioritize certain equity data as being more deterministic of a positive return on investment than others.\n",
    "\n",
    "Another outcome of these observations can be to analyze the density plot creating during the EDA phase and solely focus on the distributions of the above variables from tables 2 or 3 that are skewed towads low normalized values. The logic with this would be that outliers in negatively skewed distributions are more likely to be indicative of stronger performance because they excel in a financial ratio that very few companies excel in.\n",
    "\n",
    "The weighted scores from each analysis can be aggregated and in the end, a weighted scoring system is used.\n",
    "\n",
    "**Disclaimer: Only positive values of the correlation coefficients have been used as all negative values exist in the case of dividend-related markers which are out of scope for a growth-portfolio investing strategy, which is the focus of this analysis.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "  'toImageButtonOptions': {\n",
    "    'format': 'png',\n",
    "    'filename': 'custom_image',\n",
    "    'height': 2000,\n",
    "    'width': 2000,\n",
    "    'scale': 2\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_predictors_wide = np.array_split(top_predictors_wide, 4) # to get a 4 x 4 matrix\n",
    "title = 'Faceted Heat Map Grid of Normalized Equity Data from the Top (Left) and Bottom (Right) 20 Companies in the S&P500 Index by Predictor (1 = Best, 0 = Worst)'\n",
    "display(viz.subplot_generator(mega_df, top_predictors_wide, title=title, height_reduction_factor=15, width_multiplier=1, vertical_spacing=0.06).show(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_predictors_narrowest_adjusted = np.array_split(top_predictors_narrowest_adjusted, 2) # to get a 2 x 2 matrix\n",
    "title = 'Faceted Heat Map Grid of Normalized Equity Data from the Top (Left) and Bottom (Right) 20 Companies in the S&P500 Index by Predictor (1 = Best, 0 = Worst)'\n",
    "display(viz.subplot_generator(mega_df, top_predictors_narrowest_adjusted, title=title, height_reduction_factor=3.1, width_multiplier=1, vertical_spacing=0.06, rows=2, cols=2).show(config=config))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The left half of each heatmap subplot illustrates the score distribution of the top 20 companies sorted by the singular independent variable being observed. Conversely, the right half illustrates the bottom 20 companies under the same category. There are 16 independent variables being tested in total, those of which have been selected on the basis of having a high correlation (a positive coefficient of determination that is greater than or equal to 0.7). Some of these independent variables have a high correlation between a wider range of variables than others, which is why the contrast between the number of yellow (indicative of high scores) and blue tiles (indicative of low scores) is greater for those predictors that have a correlation with a greater number of variables.\n",
    "\n",
    "This binary contrast analysis supports the notion that there are several equity variables that are the most deterministic of the performance of an equity, with performance being interpreted as the ratio of high to low scores for each equity. Therefore, the heatmap facet grid reveals that **Market Capitalization, EBITDA (TTM), Gross Profit (MRQ and FY), Total Revenue (FY), Total Assets (MRQ), Enterprise Value (MRQ) and Net Income (FY)** are the most deterministic of the performance of an equity. Therefore, it is worth prioritizing these financial variables when determining which companies to invest in. However, it should be noted that Enterprise Value (MRQ) includes many of the above variables in its calculation, so it is essentially a derivative of several financial variables.\n",
    "\n",
    "Taking the top 4 predictors that are the most deterministic of the overall performance of an equity, constructing a binary heat map plot can enable us to better visualize the contrast between the top 20 and bottom 20 companies sorted by predictor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_predictors_narrowest_adjusted = np.concatenate(top_predictors_narrowest_adjusted)\n",
    "\n",
    "\n",
    "title = 'Binary Faceted Heat Map Grid of the Most Determinstic Equity Data for Overall Performance in the S&P500 Index (1 = Best, 0 = Worst)'\n",
    "display(viz.binary_subplot_generator(mega_df, notable_significance[0:4], title, 6.2, 1, vertical_spacing=0.03).show(config=config))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With **Market Capitalization, EBITDA (TTM), Net Income (FY) and Enterprise Value (MRQ)** being the most deterministic of overall equity performance, the strongest contrast is shown between the top 20 companies and bottom 20 companies.\n",
    "\n",
    "The last portion of this analysis addresses the **asset allocation** component the project, in which these findings are applied towards the construction of weighted variables to construct an optimized portfolio of equities. The final aspect of the portfolio construction is the use of user input, which prescribes the development of an algorithm that factors user input into the analysis algorithms developed. Therefore, the final product of the analysis uses not only the findings from the statistical algorithms devised, but also the implementation of user input withing these analysis algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_cor_cols = [\n",
    "    'Market Capitalization',\n",
    "    'EBITDA (TTM)',\n",
    "    'Gross Profit (MRQ)',\n",
    "    'Gross Profit (FY)',\n",
    "    'Total Revenue (FY)',\n",
    "    'Total Assets (MRQ)',\n",
    "    'Enterprise Value (MRQ)',\n",
    "    'Net Income (FY)'\n",
    "    ]\n",
    "low_cor_cols = [\n",
    "    'Price',\n",
    "    'Price to Earnings Ratio (TTM)',\n",
    "    'Free Cash Flow (Annual YoY Growth)',\n",
    "    'Free Cash Flow (Quarterly YoY Growth)',\n",
    "    '3-Month Performance',\n",
    "    'EPS Diluted (FY)',\n",
    "    'Debt to Equity Ratio (MRQ)',\n",
    "    'Quick Ratio (MRQ)',\n",
    "    'Dividend Yield Forward',\n",
    "    'Dividends per Share (FY)',\n",
    "    '6-Month Performance', \n",
    "    'Yearly Performance'\n",
    "    ]\n",
    "\n",
    "mega_df = mega_df[mega_df.columns.drop(list(mega_df.filter(regex='Score')))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "  'toImageButtonOptions': {\n",
    "    'format': 'png',\n",
    "    'filename': 'custom_image',\n",
    "    'height': 1100,\n",
    "    'width': 3300,\n",
    "    'scale': 2\n",
    "  }\n",
    "}\n",
    "\n",
    "viz.score_density_plot(mega_df, high_cor_cols, 'Density Plots of the Top Predictors of Equity Performance for Companies in the S&P500 Index').show(config=config)\n",
    "viz.score_density_plot(mega_df, low_cor_cols, 'Density Plot of the Weakest Predictors of Equity Performance for Companies in the S&P500 Index').show(config=config)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The density plot of the top predictors for equity performance exhbits a clear leftwards-skewed distribution trend compared to the density plot of the weakest predictors. These plots further process the excess noise from the faceted binary heat map plot by suggesting that **the top predictors for equity performance are those that most companies perform poorly on**. Therefore, being an outlier in these categories is greatly deterministic of overall performance across all other categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "equities = pf.EquityData('processed_us_equities_tradingview_data_')\n",
    "norm_df = equities.load_and_process('normalized_data', '../data/processed/')\n",
    "cols = [col + ' Score' for col in high_cor_cols]\n",
    "original_norm_df = norm_df\n",
    "\n",
    "threshold = 0.5\n",
    "for predictor in high_cor_cols:\n",
    "    norm_df = norm_df[norm_df[predictor + ' Score'] > threshold]\n",
    "\n",
    "title = f'Density Plot of the Top Performing (Individual Score > {threshold}) Companies ({len(norm_df)} total) in the S&P500 Index by the Top Predictors for Equity Performance'\n",
    "viz.score_density_plot(norm_df, cols, title, normalization=False, search_for_score=False).show(config=config)\n",
    "\n",
    "for predictor in high_cor_cols:\n",
    "    original_norm_df = original_norm_df[original_norm_df[predictor + ' Score'] < threshold]\n",
    "\n",
    "title = f'Density Plot of the Worst Performing (Individual Score < {threshold}) Companies ({len(norm_df)} total) in the S&P500 Index by the Top Predictors for Equity Performance'\n",
    "viz.score_density_plot(original_norm_df, cols, title, normalization=False, search_for_score=False).show(config=config)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first density plot proves that outliers in these financial variables are indeed the top performering companies as isolating the top performing companies from the entire data set put into the first density plot produces a mirrored image. Consequently, taking the worst performing companies produces an inverse of the first density plot, once again proving that the top predictors for equity performance are those that most companies perform poorly on."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
