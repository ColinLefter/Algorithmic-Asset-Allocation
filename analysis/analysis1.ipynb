{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Colin Lefter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Research question/interests\n",
    "\n",
    "**What equity data is the most deterministic of the price of an equity such that we can compute an optimized portfolio of equities while using user input to drive our optimization algorithm?**\n",
    "\n",
    "My research objective is to develop a scalable asset allocation and construction algorithm that implements an objected-oriented design approach. This objective is an outcome of determining what equity data is the most deterministic of the price of an equity, which will be the focus for the majority of the project/\n",
    "\n",
    "I intend to develop algorithms for constructing multiple linear regressions and Fourier Franforms, among others, that I will then use to construct interactive and statistical models with Plotly and Seaborn. As such, I have a strong interest in the system design of our software and in developing helper functions that can assist all of us with processing data more efficiently. I am also looking forward to using Facebook Prophet[^1] to construct a time series forecast of a sample portfolio recommendation from our software, which can be included in our Tableau Dashboard.\n",
    "\n",
    "### Analysis Plan\n",
    "Our objective function is one that takes in a selection of columns from our data sets to then search for the top n companies that satisfy a criteria for having the highest probability of producing an optimal return on investment. These inputs themselves refer to sub-objective functions that take as input user-defined parameters and thresholds that set the criteria for favourable performance attributes. To rank the companies from our data set, and ultimately determine what portion of capital to assign to each equity, I propose a data normalization algorithm that normalizes the data that comprises the favourable subset from each column of our data set. We interpret these normalized values as probabilities of equity selection and ultimately average the score of each company across all columns to then multiply the final score percentage of each company with the total capital specified by the user. In a broad sense, our software is composed of four general classes that include \"Data\", \"Quantitative Analysis\", \"Data Visualization\" and \"Portfolio Construction\". We inherit the properties from each of these classes to build a functional data analysis chain.\n",
    "\n",
    "Our data visualization will be concerned with analyzing the influence of certain financial variables, such as Price-to-Earnings, on the price of each equity from a sample of 500 equities (from the S&P 500 index). Such analysis would begin with a statistical summary that will constitute exploratory data analysis, followed by our application of analysis algorithms that we design. The construction of a portfolio is a bonus of our project and will be made possible by the analysis algorithms we have constructed.\n",
    "\n",
    "**Important Note**\n",
    "A component of the analysis will involve the comparison of different values of financial variables with the corresponding price of each equity. This constitutes inferential analysis as we are attempting to identify a correlation on the basis of picking stocks based on expected performance. Therefore, this will require us to use past financial data and compare this data with the current price of each equity. As a result, we can only use the 3-month performance data (i.e. 3-month change in share price data) for this comparison as otherwise we would be using future data to predict past performance, which would be invalid.\n",
    "\n",
    "#### User-defined parameters\n",
    "Some initial ideas for these parameters include:\n",
    "- (float) Initial capital\n",
    "- (float) Additional capital per day, week or month\n",
    "- (int) Intended holding period (in days)\n",
    "- (boolean) Importance of dividends (validated based on capital invested)\n",
    "- (String) Preferred industries (choose from a list, or select all)\n",
    "- (int) Volatility tolerance (from 0 to 1, 1 indicating that volatility is not important)\n",
    "- (String) Preferred companies (as a list)[^2]\n",
    "- (int) Preferred degree of portfolio diversification (from 0 to 1, 1 indicating complete diversification)\n",
    "- (String) Preferred investment strategy (choose from \"Growth\", \"Value\", \"GARP\")\n",
    "\n",
    "### Algorithm Plan\n",
    "\n",
    "####  Tier 1: Threshold-based screening algorithms\n",
    "- The current plan is to use these algorithms to screen the financial documents from each company by setting a minimum threshold for each financial ratio. This class of algorithms will need to conduct such screening per industry as industry financial ratios are dinstinct from one another.\n",
    "- A global screening algorithm that selects companies which show favourable performance across all ratios can also be used after each ratio has been individually tested.\n",
    "\n",
    "#### Tier 2: Regression models\n",
    "- As of now, the intent is to develop a multiple linear regression model that will attempt to determine a relationship between the yearly and quarterly performance of each company in relation to several columns of data that act as predictors. This can essentially implement the results from the threshold-based screening algorithms to only conduct this analysis on the pre-screened companies.\n",
    "\n",
    "#### Tier 3: Statistical modelling algorithms\n",
    "- Tier 3 denotes a class of broadly experimental statistical modelling algorithms that are applied on a pre-final portfolio to add additional points to companies that perform exceptionally well compared to others in the portfolio. For now, these algorithms constitute signal processing algorithms such as a Fourier Transform algorithm that attempts to identify peaks in numerical values that would otherwise not be apparent when examined in isolation and without further processing. Therefore, these algorithms will be used to fine-tune the capital allocation percentages for each company in the pre-final portfolio.\n",
    "\n",
    "#### Columns of relevance\n",
    "Data set 1: Overview\n",
    "- Price\n",
    "- MKT Cap\n",
    "- P/E\n",
    "- EPS\n",
    "- Sector\n",
    "\n",
    "Data set 2: Performance\n",
    "- 1M change (1 month change)\n",
    "- 3-Month performance\n",
    "- 6-month perfromance\n",
    "- YTD performance\n",
    "- Yearly performance\n",
    "- Volatility\n",
    "\n",
    "Data set 3: Valuation\n",
    "- Price / revenue\n",
    "- Enterprise value\n",
    "\n",
    "Data set 4: Dividends\n",
    "- Dividend yield FWD\n",
    "- Dividends per share (FY)\n",
    "\n",
    "Data set 5: Margins\n",
    "- Gross profit margin\n",
    "- Operating margin\n",
    "- Net profit margin\n",
    "\n",
    "Data set 6: Income Statement\n",
    "- Gross profit\n",
    "- Income\n",
    "- Net cash flow\n",
    "\n",
    "Data set 7: Balance Sheet\n",
    "- Current ratio\n",
    "- Debt/equity\n",
    "- Quick ratio\n",
    "\n",
    "The total number of columns would be 24 in this case.\n",
    "\n",
    "[^1]: This would mean that a few time series data sets would need to be downloaded from TradingView at the end of the project to test the demo porfolio.\n",
    "\n",
    "[^2]: A helper function can be developed for this, where the user can just type out the name of the company and the ticker is identifed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "balance_sheet_df = pd.read_csv(\"../data/raw/us_equities_tradingview_data_balance_sheet.csv\")\n",
    "dividends_df = pd.read_csv(\"../data/raw/us_equities_tradingview_data_dividends.csv\")\n",
    "income_statement_df = pd.read_csv(\"../data/raw/us_equities_tradingview_data_income_statement.csv\")\n",
    "margins_df = pd.read_csv(\"../data/raw/us_equities_tradingview_data_margins.csv\")\n",
    "overview_df = pd.read_csv(\"../data/raw/us_equities_tradingview_data_overview.csv\")\n",
    "performance_df = pd.read_csv(\"../data/raw/us_equities_tradingview_data_performance.csv\")\n",
    "valuation_df = pd.read_csv(\"../data/raw/us_equities_tradingview_data_valuation.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
