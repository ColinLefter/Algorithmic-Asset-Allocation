{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Colin Lefter"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Research question/interests\n",
    "\n",
    "**What equity data is the most deterministic of the price of an equity, and of this data, which is the most relevant for a growth portfolio investment strategy such that we can compute an optimized portfolio of equities while using user input to drive our optimization algorithm?**\n",
    "\n",
    "My research objective is to develop a scalable asset allocation and construction algorithm that implements an objected-oriented design approach. This objective is an outcome of determining what equity data is the most deterministic of the price of an equity, which will be the focus for the majority of the project/\n",
    "\n",
    "I intend to develop algorithms for constructing multiple linear regressions and Fourier Franforms, among others, that I will then use to construct interactive and statistical models with Plotly and Seaborn. As such, I have a strong interest in the system design of our software and in developing helper functions that can assist all of us with processing data more efficiently. I am also looking forward to using Facebook Prophet[^1] to construct a time series forecast of a sample portfolio recommendation from our software, which can be included in our Tableau Dashboard.\n",
    "\n",
    "### Analysis Plan\n",
    "Our objective function is one that takes in a selection of columns from our data sets to then search for the top n companies that satisfy a criteria for having the highest probability of producing an optimal return on investment. These inputs themselves refer to sub-objective functions that take as input user-defined parameters and thresholds that set the criteria for favourable performance attributes. To rank the companies from our data set, and ultimately determine what portion of capital to assign to each equity, I propose a data normalization algorithm that normalizes the data that comprises the favourable subset from each column of our data set. We interpret these normalized values as probabilities of equity selection and ultimately average the score of each company across all columns to then multiply the final score percentage of each company with the total capital specified by the user. In a broad sense, our software is composed of four general classes that include \"Data\", \"Quantitative Analysis\", \"Data Visualization\" and \"Portfolio Construction\". We inherit the properties from each of these classes to build a functional data analysis chain.\n",
    "\n",
    "Our data visualization will be concerned with analyzing the influence of certain financial variables, such as Price-to-Earnings, on the price of each equity from a sample of 500 equities (from the S&P 500 index). Such analysis would begin with a statistical summary that will constitute exploratory data analysis, followed by our application of analysis algorithms that we design. The construction of a portfolio is a bonus of our project and will be made possible by the analysis algorithms we have constructed.\n",
    "\n",
    "**Important Note**\n",
    "A component of the analysis will involve the comparison of different values of financial variables with the corresponding price of each equity. This constitutes inferential analysis as we are attempting to identify a correlation on the basis of picking stocks based on expected performance. Therefore, this will require us to use past financial data and compare this data with the current price of each equity. As a result, we can only use the 3-month performance data (i.e. 3-month change in share price data) for this comparison as otherwise we would be using future data to predict past performance, which would be invalid.\n",
    "\n",
    "#### User-defined parameters\n",
    "Some initial ideas for these parameters include:\n",
    "- (float) Initial capital\n",
    "- (float) Additional capital per day, week or month\n",
    "- (int) Intended holding period (in days)\n",
    "- (boolean) Importance of dividends (validated based on capital invested)\n",
    "- (String) Preferred industries (choose from a list, or select all)\n",
    "- (int) Volatility tolerance (from 0 to 1, 1 indicating that volatility is not important)\n",
    "- (String) Preferred companies (as a list)[^2]\n",
    "- (int) Preferred degree of portfolio diversification (from 0 to 1, 1 indicating complete diversification)\n",
    "- (String) Preferred investment strategy (choose from \"Growth\", \"Value\", \"GARP\")\n",
    "\n",
    "### Algorithm Plan\n",
    "\n",
    "####  Tier 1: Threshold-based screening algorithms\n",
    "- The current plan is to use these algorithms to screen the financial documents from each company by setting a minimum threshold for each financial ratio. This class of algorithms will need to conduct such screening per industry as industry financial ratios are dinstinct from one another.\n",
    "- A global screening algorithm that selects companies which show favourable performance across all ratios can also be used after each ratio has been individually tested.\n",
    "\n",
    "#### Tier 2: Regression models\n",
    "- As of now, the intent is to develop a multiple linear regression model that will attempt to determine a relationship between the yearly and quarterly performance of each company in relation to several columns of data that act as predictors. This can essentially implement the results from the threshold-based screening algorithms to only conduct this analysis on the pre-screened companies.\n",
    "\n",
    "#### Tier 3: Statistical modelling algorithms\n",
    "- Tier 3 denotes a class of broadly experimental statistical modelling algorithms that are applied on a pre-final portfolio to add additional points to companies that perform exceptionally well compared to others in the portfolio. For now, these algorithms constitute signal processing algorithms such as a Fourier Transform algorithm that attempts to identify peaks in numerical values that would otherwise not be apparent when examined in isolation and without further processing. Therefore, these algorithms will be used to fine-tune the capital allocation percentages for each company in the pre-final portfolio.\n",
    "\n",
    "#### Columns of relevance\n",
    "Data set 1: Overview\n",
    "- Price\n",
    "- MKT Cap\n",
    "- P/E\n",
    "- EPS\n",
    "- Sector\n",
    "\n",
    "Data set 2: Performance\n",
    "- 1M change (1 month change)\n",
    "- 3-Month performance\n",
    "- 6-month perfromance\n",
    "- YTD performance\n",
    "- Yearly performance\n",
    "- Volatility\n",
    "\n",
    "Data set 3: Valuation\n",
    "- Price / revenue\n",
    "- Enterprise value\n",
    "\n",
    "Data set 4: Dividends\n",
    "- Dividend yield FWD\n",
    "- Dividends per share (FY)\n",
    "\n",
    "Data set 5: Margins\n",
    "- Gross profit margin\n",
    "- Operating margin\n",
    "- Net profit margin\n",
    "\n",
    "Data set 6: Income Statement\n",
    "- Gross profit\n",
    "- Income\n",
    "- Net cash flow\n",
    "\n",
    "Data set 7: Balance Sheet\n",
    "- Current ratio\n",
    "- Debt/equity\n",
    "- Quick ratio\n",
    "\n",
    "The total number of columns would be 24 in this case.\n",
    "\n",
    "[^1]: This would mean that a few time series data sets would need to be downloaded from TradingView at the end of the project to test the demo porfolio.\n",
    "\n",
    "[^2]: A helper function can be developed for this, where the user can just type out the name of the company and the ticker is identifed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import preprocessing\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import plotly.figure_factory as ff\n",
    "from IPython.display import display, HTML, Markdown, Latex\n",
    "from tqdm import tqdm, trange\n",
    "from typing import *\n",
    "from dataclasses import dataclass\n",
    "from scipy import stats\n",
    "import plotly.io as pio\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from analysis.code import project_functions1 as pf\n",
    "#import plotly.offline as pyo\n",
    "\n",
    "#pio.renderers.default = \"svg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ValueRange:\n",
    "    min: float\n",
    "    max: float\n",
    "    \n",
    "    def validate(self, x):\n",
    "        \"\"\"Checks if inputs to variables that must lie within a specific range are valid\n",
    "        \n",
    "        :x: the value that must be checked as satisfying the specified range\n",
    "        :raises ValueError: if the value does not lie within the specified range\n",
    "        \"\"\"\n",
    "        if not (self.min <= x <= self.max):\n",
    "            raise ValueError(f'{x} must be between 0 and 1 (including).')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "equities = pf.EquityData()\n",
    "overview_df = equities.load_and_process(\"overview\", exclude_columns=['Change %', 'Change', 'Technical Rating', 'Volume', 'Volume*Price'])\n",
    "income_statement_df = equities.load_and_process(\"income_statement\")\n",
    "balance_sheet_df = equities.load_and_process(\"balance_sheet\")\n",
    "dividends_df = equities.load_and_process(\"dividends\", exclude_columns=['Price'])\n",
    "margins_df = equities.load_and_process(\"margins\")\n",
    "performance_df = equities.load_and_process(\"performance\", exclude_columns=['Change 1m, %', 'Change 5m, %', 'Change 15m, %', 'Change 1h, %', 'Change 4h, %', 'Change 1W, %', 'Change 1M, %', 'Change %'])\n",
    "valuation_df = equities.load_and_process(\"valuation\", exclude_columns=['Price', 'Market Capitalization', 'Price to Earnings Ratio (TTM)', 'Basic EPS (TTM)', 'EPS Diluted (FY)'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis-Specific Data Wrangling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = [\n",
    "    overview_df,\n",
    "    income_statement_df,\n",
    "    balance_sheet_df,\n",
    "    dividends_df,\n",
    "    margins_df,\n",
    "    performance_df,\n",
    "    valuation_df\n",
    "    ]\n",
    "\n",
    "dfs_names = [\n",
    "    \"Overview Data\",\n",
    "    \"Balance Sheet Data\",\n",
    "    \"Dividends Data\",\n",
    "    \"Income Statement Data\",\n",
    "    \"Margins Data\",\n",
    "    \"Performance Data\",\n",
    "    \"Valuation Data\"\n",
    "    ]\n",
    "\n",
    "overview_df['3-Month Performance'] = performance_df['3-Month Performance']\n",
    "income_statement_df['3-Month Performance'] = performance_df['3-Month Performance']\n",
    "balance_sheet_df['3-Month Performance'] = performance_df['3-Month Performance']\n",
    "dividends_df['3-Month Performance'] = performance_df['3-Month Performance']\n",
    "margins_df['3-Month Performance'] = performance_df['3-Month Performance']\n",
    "valuation_df['3-Month Performance'] = performance_df['3-Month Performance']\n",
    "\n",
    "mega_df = pd.concat(dfs, axis=1)\n",
    "mega_df = mega_df.loc[:,~mega_df.columns.duplicated()].copy()\n",
    "mega_df = mega_df.dropna()\n",
    "mega_df_no_strings = mega_df.select_dtypes(exclude='object')\n",
    "\n",
    "mega_df['6-Month Performance'] = performance_df['6-Month Performance']\n",
    "mega_df['YTD Performance'] = performance_df['YTD Performance']\n",
    "mega_df['Yearly Performance'] = performance_df['Yearly Performance']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functional Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantitativeAnalysis:\n",
    "    def __init__(self, number_of_companies: int=500, initial_capital: float=100000.00, capital_per_period: float=100.00, period: int=7, dividends_importance: bool=False, preferred_industries: list=[\"Technology Services, Electronic Technology\"],\n",
    "                volatility_tolerance: Annotated[float, ValueRange(0.0, 1.0)]=0.7, preferred_companies: list=[\"Apple, Google, Microsoft, Amazon\"], diversification: Annotated[float, ValueRange(0.0, 1.0)]=0.4, investment_strategy: str=\"Growth\"):\n",
    "        \"\"\"Includes several analysis functions that process select data across all data sets\n",
    "\n",
    "        :number_of_companies: the number of companies included in the sample, with the default being those from the S&P500 Index\n",
    "        :initial_capital: the initial amount of cash to be invested by the client, in USD\\n\n",
    "        :capital_per_period: the amount of cash to be invested by the client at a fixed rate in addition to the initial capital invested, in USD\\n\n",
    "        :period: the frequency (in days) at which additional cash is invested, if desired\\n\n",
    "        :dividends_importance: specifies whether dividends are important to the client, dictating whether analysis algorithms should place greater importance on dividends\\n\n",
    "        :preferred_industries: specifies a list of industries that the analysis algorithms should prioritize when constructing the investment portfolio\\n\n",
    "        :volatility_tolerance: accepts a range of values from 0 to 1, with 1 implying maximum volatility tolerance (i.e. the client is willing to lose 100% of their investment to take on more risk)\\n\n",
    "        :preferred_companies: specifies a list of companies that the analysis algorithms will accomodate in the final portfolio irrespective of their score\\n\n",
    "        :diversification: accepts a range of values from 0 to 1, with 1 implying maximum diversification (i.e. funds will be distributed evenly across all industries and equally among all companies)\\n\n",
    "        :investment_strategy: specifies the investment strategy that will guide the output of the analysis algorithms, in which this analysis notebook strictly focuses on growth investing\\n\n",
    "        :raises: ValueError if an input parameter does not satisfy its accepted range\n",
    "        \"\"\"\n",
    "        \n",
    "        self.number_of_companies = number_of_companies\n",
    "        self.initial_capital = initial_capital\n",
    "        self.capital_per_period = capital_per_period\n",
    "        self.period = period\n",
    "        self.dividends_importance = dividends_importance\n",
    "        self.preferred_industries = preferred_industries\n",
    "        self.volatility_tolerance = volatility_tolerance\n",
    "        self.preferred_companies = preferred_companies\n",
    "        self.diversification = diversification\n",
    "        self.preferred_companies = preferred_industries\n",
    "        self.investment_strategy = investment_strategy\n",
    "        \n",
    "    def lin_reg_coef_determination(self, df: pd.DataFrame, X: str, y: str='3-Month Performance', filter_outliers: bool=True):\n",
    "        if filter_outliers:\n",
    "            df = self.outlier_filtered_df(df, col=y)\n",
    "        \n",
    "        X = df[X]\n",
    "        y = df[y]\n",
    "        \n",
    "        y = y.dropna()\n",
    "        X = X.dropna()\n",
    "        \n",
    "        if len(X) > len(y):\n",
    "            X = X[:len(y)]\n",
    "        else:\n",
    "            y = y[:len(X)]\n",
    "        \n",
    "        self.X = np.array(X).reshape(-1, 1)\n",
    "        self.y = np.array(y).reshape(-1, 1)\n",
    "        \n",
    "        model = LinearRegression()\n",
    "        model.fit(self.X, self.y)\n",
    "         \n",
    "        return model.score(self.X, self.y)\n",
    "\n",
    "    def get_lin_reg_coefs(self, df: pd.DataFrame, x_values: list(), y_value: str='3-Month Performance') -> pd.DataFrame:\n",
    "        \"\"\"Returns a Pandas DataFrame with the coefficients of determination for each y-on-x regression\n",
    "        Example: 3-Month Performance against Price to Earnings Ratio (TTM)\n",
    "        \n",
    "        :df: the data frame that contains the columns to process\n",
    "        :x_values: a list of strings of the names of each column to process\\n\n",
    "        :y_value: a common y-value to map each x value against in the regression analysis\\n\n",
    "        :returns: A Pandas DataFrame with the coefficients of determination for each y-on-x regression\\n\n",
    "        \n",
    "        \"\"\"\n",
    "        self.coef_dict = dict.fromkeys(x_values, 0) # initialize a dict with all the columns assigned to a value of 0\n",
    "        \n",
    "        for predictor in tqdm(x_values, desc=\"Constructing linear regression models\", total=len(x_values)):\n",
    "            self.coef_dict[predictor] = self.lin_reg_coef_determination(df, X=predictor, y=y_value)\n",
    "        \n",
    "        self.processed_df = pd.DataFrame(list(zip(self.coef_dict.keys(), self.coef_dict.values())), columns=[f'Equity Data Against {y_value}', 'Coefficient of Determination'])\n",
    "        \n",
    "        return self.processed_df\n",
    "        \n",
    "    def fourier_transform(self):\n",
    "        pass\n",
    "    \n",
    "    def rank(self, df: pd.DataFrame, col: str, normalize_only: bool=True, threshold: float=1.5,\n",
    "             below_threshold: bool=True, filter_outliers: bool=True, normalize_after: bool=False,\n",
    "             lower_quantile: float=0.05, upper_quantile: float=0.95):\n",
    "        \"\"\"The scoring algorithm for determining the weight of each equity in the construction of the portfolio for this specific column examined.\n",
    "        Features a custom outlier-filtering algorithm that is robust to outliers in the data set while still returning normalized values.\n",
    "        \n",
    "        :df: The original dataframe\\n\n",
    "        :col: The name of the column being extracted from the dataframe provided\\n\n",
    "        :normalize_only: if True, does not apply a threshold to the screening algorithm, and only normalizes values with a minmax scaler\\n\n",
    "        :threshold: the minimum value that equities must have for that column in order to be considered for further analysis\\n\n",
    "        :below_threshold: if True, removes equities that are below the threshold for that column\\n\n",
    "        :filter_outliers: if True, does not consider equities in the data normalization algorithm, but assigns a min or max value to all outliers depending on the below_threshold parameter\\n\n",
    "        :normalize_after: if True, normalizes the data only after the threshold filter has been applied\\n\n",
    "        :lower_quantile: specifies the lower quantile of the distribution when filtering outliers\\n\n",
    "        :upper_quantile: specifies the upper quantile of the distribution when filtering outliers\\n\n",
    "        \"\"\"\n",
    "        \n",
    "        #NOTE: should make an option for no threshold\n",
    "        self.x = df[col]\n",
    "        new_col = col + \" Score\"\n",
    "        \n",
    "        # normalization can be done either before or after equities have been filtered by the threshold\n",
    "        # the difference is that by filtering initially, the min and max values of that smaller set will become 0 and 1 respectively\n",
    "        df[new_col] = np.NaN # initialize the score column with only NaN values\n",
    "        \n",
    "        def outlier_filter(self):\n",
    "            \"\"\"Nested helper function to filter outliers\"\"\"\n",
    "            upper_fence = self.x.quantile(upper_quantile)\n",
    "            lower_fence = self.x.quantile(lower_quantile)\n",
    "            \n",
    "            if below_threshold:\n",
    "                df.loc[self.x > upper_fence, new_col] = 1 # outliers still need to be included in the data (max score assigned)\n",
    "                df.loc[self.x < lower_fence, new_col] = 0 # lowest score assigned\n",
    "            else:\n",
    "                # inverse of the above\n",
    "                df.loc[self.x > upper_fence, new_col] = 0\n",
    "                df.loc[self.x < lower_fence, new_col] = 1\n",
    "\n",
    "            # now only take the rows that are not outliers into the minmax scaler\n",
    "            self.x = self.x[(self.x <= upper_fence) & (self.x >= lower_fence)]\n",
    "            \n",
    "            if normalize_only:\n",
    "                normalize_after = False\n",
    "                \n",
    "            if normalize_after:\n",
    "                if below_threshold:\n",
    "                    # since we are only taking valid values, we consider the inverse of the values that are below the threshold to be valid values\n",
    "                    self.x = self.x[self.x >= threshold]\n",
    "                else:\n",
    "                    self.x = self.x[self.x <= threshold]\n",
    "        \n",
    "        if filter_outliers:\n",
    "            outlier_filter(self)\n",
    "        \n",
    "        self.y = np.array(self.x).reshape(-1, 1)\n",
    "        self.y = preprocessing.MinMaxScaler().fit_transform(self.y)\n",
    " \n",
    "        for col_idx, array_idx in zip(self.x.index, range(len(self.y))):\n",
    "            df.at[col_idx, new_col] = self.y[array_idx]\n",
    "        \n",
    "        # if we are giving the minimum score to values below the threshold, assign 0 to those values\n",
    "        if not normalize_only:\n",
    "            if below_threshold:\n",
    "                df.loc[df[col] <= threshold, new_col] = 0\n",
    "            else:\n",
    "                df.loc[df[col] >= threshold, new_col] = 0\n",
    "    \n",
    "    def outlier_filtered_df(self, df: pd.DataFrame, col: str, lower_quantile: float=0.05, upper_quantile: float=0.95):\n",
    "        upper_fence = df[col].quantile(upper_quantile)\n",
    "        lower_fence = df[col].quantile(lower_quantile)\n",
    "\n",
    "        df = df[(df[col] <= upper_fence) & (df[col] >= lower_fence)]\n",
    "        \n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataVisualization(QuantitativeAnalysis):\n",
    "    def __init__(self):\n",
    "        QuantitativeAnalysis.__init__(self)\n",
    "\n",
    "    def score_distribution_plot(self, df: pd.DataFrame, data_name: str):\n",
    "        df = df.select_dtypes(exclude='object')[:self.number_of_companies]\n",
    "        self.n = len(df)\n",
    "        \n",
    "        for column in df.columns:\n",
    "            self.rank(df, col=column, upper_quantile=0.99, lower_quantile=0.01)\n",
    "            \n",
    "        self.score_data_length = len(df.axes[1])\n",
    "        self.input_df = df.T[int(self.score_data_length/2 + 1):].T\n",
    "        self.hist_data = [self.input_df[x] for x in self.input_df.columns]\n",
    "        \n",
    "        self.group_labels = [x for x in self.input_df.columns]\n",
    "        self.colors = ['#333F44', '#37AA9C', '#94F3E4']\n",
    "\n",
    "        self.fig = ff.create_distplot(self.hist_data, self.group_labels, show_hist=False, colors=self.colors)\n",
    "        self.fig.update_layout(title_text=f'Distribution for Normalized {data_name} of {self.n} Companies in the S&P500', template='plotly_dark')\n",
    "        \n",
    "        self.fig.update_xaxes(title=\"Score (0 = low, 1 = high)\")\n",
    "        self.fig.update_yaxes(title=\"Count\")\n",
    "        \n",
    "        return self.fig\n",
    "\n",
    "    def heatmap_plot(self, df: pd.DataFrame, data_name: str, number_of_companies: int):\n",
    "        df = df[:number_of_companies]\n",
    "        self.z = []\n",
    "        self.tickers = df['Ticker']\n",
    "        df.index = df['Ticker']\n",
    "        df = df.select_dtypes(exclude='object')\n",
    "        for column in df.columns:\n",
    "            self.rank(df, col=column)\n",
    "\n",
    "        self.score_data_length = len(df.axes[1])\n",
    "        self.input_df = df.T[int(self.score_data_length/2 + 1):].T\n",
    "        for column in self.input_df.columns:\n",
    "            self.z.append(self.input_df[column].round(3))\n",
    "        \n",
    "        # add text_auto=True as a parameter back to this function once latest Plotly update is installed\n",
    "        self.title = f'Heat Map of Normalized {data_name} for the Top {number_of_companies} Companies by Market Capitalization in the S&P500 Index'\n",
    "        \n",
    "        fig = px.imshow(\n",
    "            self.z, text_auto=True, template='plotly_dark', title=self.title, x=[x for x in self.tickers], y=[x for x in df.columns[int(self.score_data_length/2 + 1):]]\n",
    "                )\n",
    "        \n",
    "        return fig\n",
    "\n",
    "    def scatter_3d(self, df, x, y, z):\n",
    "        df.index = df['Ticker']\n",
    "        df = df.select_dtypes(exclude='object')\n",
    "        \n",
    "        for column in df.columns:\n",
    "            self.rank(df, col=column)\n",
    "            \n",
    "        fig = px.scatter_3d(df, x=x, y=y, z=z,\n",
    "                    title='3D Scatter Plot of Normalized Equity Data',\n",
    "                    template='plotly_dark',\n",
    "                    size_max=18,\n",
    "                    color='3-Month Performance Score',\n",
    "                    opacity=0.7)\n",
    "\n",
    "        return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PortfolioConstruction(DataVisualization):\n",
    "    def __init__(self):\n",
    "        DataVisualization.__init__(self)\n",
    "    \n",
    "    def asset_allocation(self):\n",
    "        pass\n",
    "    \n",
    "    def construct_portfolio(self):\n",
    "        pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimental Feature Development Zone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quant = QuantitativeAnalysis()\n",
    "viz = DataVisualization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "  'toImageButtonOptions': {\n",
    "    'format': 'png',\n",
    "    'filename': 'custom_image',\n",
    "    'height': 800,\n",
    "    'width': 2000,\n",
    "    'scale': 2\n",
    "  }\n",
    "}\n",
    "\n",
    "all_columns = list(mega_df_no_strings.columns)\n",
    "all_columns.remove('Price')\n",
    "all_columns.remove('3-Month Performance')\n",
    "all_columns.remove('6-Month Performance')\n",
    "all_columns.remove('YTD Performance')\n",
    "all_columns.remove('Yearly Performance')\n",
    "all_columns.remove('1-Year Beta')\n",
    "y_value='3-Month Performance'\n",
    "\n",
    "y_values=['3-Month Performance', 'YTD Performance']\n",
    "for y_value in y_values:\n",
    "    df = quant.get_lin_reg_coefs(mega_df, x_values=all_columns, y_value=y_value)\n",
    "    fig = px.bar(df, x=f'Equity Data Against {y_value}', y='Coefficient of Determination')\n",
    "    fig.update_layout(title_text=f'Coefficients of Determination for Equity Data Against {y_value}', template='plotly_dark')\n",
    "    #fig.show(config=config)\n",
    "\n",
    "#z = quant.lin_reg_coef_determination(overview_df, x='Price to Earnings Ratio (TTM)')\n",
    "#fig = px.scatter(quant.outlier_filtered_df(overview_df, col=\"Price to Earnings Ratio (TTM)\"), x=\"Price to Earnings Ratio (TTM)\", y=\"3-Month Performance\", trendline=\"ols\")\n",
    "#fig.show()\n",
    "\n",
    "#overview_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img1](../images/img_1.png)\n",
    "![img1](../images/img_2.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As anticipated, the coefficients of determination are extremely low and no correlation is evident. Therefore, the next step is to construct a multiple linear regression model that will only select the predictors that have the highest correlation from the tests conducted above. The goal of this analysis is to normalize the coefficients of determination and use that as a multiplier to the default ranked scores applied to each equity data column."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis Zone\n",
    "## Note: to view the interactive graphs plotted, run this analysis notebook in a Jupyter Notebook environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for df, name in zip(dfs, dfs_names):\n",
    "    #df.dropna(inplace=True)\n",
    "    #viz.score_distribution_plot(df, name).show(config=config)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img3](../images/img_3.png)\n",
    "![img4](../images/img_4.png)\n",
    "![img5](../images/img_5.png)\n",
    "![img6](../images/img_6.png)\n",
    "![img7](../images/img_7.png)\n",
    "![img8](../images/img_8.png)\n",
    "![img9](../images/img_9.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These plots reveal a number of things. First, they indicate that there are cases where most companies perform either reasonably well or generally bad for certian financial ratios, as represented by the width of the distribution plot for each ratio. The smaller it is, the more likely that most companies perform similarly. Secondly, most graphs have cases where outliers exist for each category, in which these companies may perform exceptionally well compared to others for a certain financial ratio--being a potentially strong pick.\n",
    "\n",
    "The next step is to investigate if a correlation exists between cases where most companies score low for a certain ratio, and outliers of that segment performing exceptionally well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#viz.heatmap_plot(mega_df, f'Complete Equity Data ({len(mega_df.columns)} Data Points)', 50)\n",
    "\n",
    "#for df, names in zip(dfs, dfs_names):\n",
    "    #viz.heatmap_plot(df, names, 50).show(config=config)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img10](../images/img_10.png)\n",
    "![img11](../images/img_11.png)\n",
    "![img12](../images/img_12.png)\n",
    "![img13](../images/img_13.png)\n",
    "![img14](../images/img_14.png)\n",
    "![img15](../images/img_15.png)\n",
    "![img16](../images/img_16.png)\n",
    "![img18](../images/img_18.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These heat plots reveal that for certain categories of data, particularly valuation, income statement and balance sheet data, the top companies by market capitalization tend to have the highest scores in those categories. Although this may indicate that such companies with the highest aggregated normalized scores are the best pick for an investment portfolio, this assumption must be validated against their past 3-month performance to see if these scores did indeed dictate a positive change in the price of an equity--indicating a positive return on investment. A multiple linear regression can be used, but first, a 3D plot can be used to closely analyze the correlation of two of the most important pieces of equity data when picking stocks with the corresponding change in the price of such stocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz.scatter_3d(mega_df, 'Price to Earnings Ratio (TTM) Score', 'Free Cash Flow Margin (FY) Score', '3-Month Performance Score').show(config=config)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img19](../images/img_19.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There appears to be a reasonably strong correlation between the normalized Free Cash Flow Margin (FY) score and the corresponding 3-Month Performance score, indicating that a high Free Cash FLow Margin (FY) Score may be a good metric to consider when picking a stock. The same applies for the correlation between the Price to Earnings Ratio (TTM) score and the 3-Month Performance score. Plotting a regression line can be used to validate this hypothesis."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
